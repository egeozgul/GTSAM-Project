{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e810f3",
   "metadata": {},
   "source": [
    "# Testing the scaling limits of GTSAM on 1dsfm dataset\n",
    "\n",
    "![image](media/1dsfmRef.png)\n",
    "\n",
    "The dataset can be found here: https://www.cs.cornell.edu/projects/1dsfm/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad4613",
   "metadata": {},
   "source": [
    "## Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293368ab",
   "metadata": {},
   "source": [
    "###  Preprocessing Pipeline\n",
    "\n",
    "Before performing Bundle Adjustment (BA) scaling experiments in GTSAM, the raw, unordered image collections from the **1DSfM dataset** must be transformed into a structured factor graph. We utilize **COLMAP**, a state-of-the-art Structure-from-Motion (SfM) pipeline, as the frontend to achieve this. The \n",
    "\n",
    "### Why is this step necessary?\n",
    "\n",
    "GTSAM is a backend optimization library; it requires a defined **NonlinearFactorGraph** and good **Initial Values** to converge. COLMAP solves three critical problems:\n",
    "\n",
    "1.  **Data Association (Feature Matching):** We must determine which 2D pixel $(u,v)$ in Image $A$ corresponds to the same 3D landmark as pixel $(u',v')$ in Image $B$. COLMAP uses SIFT descriptors and geometric verification to generate these feature tracks.\n",
    "2.  **Initialization (Sparse Reconstruction):** Bundle Adjustment is a non-convex optimization problem. If initialized with identity poses and zero-point coordinates, the optimizer will get stuck in local minima. COLMAP provides a robust initial estimate for Camera Poses $(R, t)$ and 3D Points $(X, Y, Z)$.\n",
    "3.  **Graph Topology:** The reconstruction determines the sparsity pattern of the graphâ€”defining exactly which cameras observe which landmarks.\n",
    "\n",
    "---\n",
    "\n",
    "### The Execution Pipeline\n",
    "\n",
    "The preprocessing was executed via a shell script using a Dockerized version of COLMAP (for GPU acceleration). The pipeline consists of four sequential stages:\n",
    "\n",
    "```bash\n",
    "# 1. Feature Extraction\n",
    "# Detects SIFT features. 'single_camera 0' allows for varying image dimensions.\n",
    "colmap feature_extractor \\\n",
    "    --database_path database.db \\\n",
    "    --image_path /images \\\n",
    "    --ImageReader.single_camera 0\n",
    "\n",
    "# 2. Feature Matching\n",
    "# Associates features across images. We utilize a Vocabulary Tree (FAISS) \n",
    "# for O(N) efficiency on large datasets (Montreal/Piccadilly).\n",
    "colmap vocab_tree_matcher \\\n",
    "    --database_path database.db \\\n",
    "    --VocabTreeMatching.vocab_tree_path vocab_tree_faiss.bin\n",
    "\n",
    "# 3. Sparse Reconstruction (The SfM Frontend)\n",
    "# Incrementally registers images to build the initial 3D model.\n",
    "colmap mapper \\\n",
    "    --database_path database.db \\\n",
    "    --image_path /images \\\n",
    "    --output_path sparse\n",
    "\n",
    "# 4. Conversion to Bundler Format\n",
    "# Exports the binary COLMAP model to a text-based format parseable by our scripts.\n",
    "colmap model_converter \\\n",
    "    --input_path sparse/0 \\\n",
    "    --output_path colmap_bundle.out \\\n",
    "    --output_type Bundler\n",
    "\n",
    "### ðŸ“‚ Output Artifacts\n",
    "\n",
    "The pipeline generates two critical files that serve as the interface between the COLMAP frontend and the GTSAM backend:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433445e9",
   "metadata": {},
   "source": [
    "\n",
    "| File Name | Content Description | Role in GTSAM |\n",
    "| :--- | :--- | :--- |\n",
    "| **`colmap_bundle.out.bundle.out`** | **The Geometry File** (Bundler Format)<br>â€¢ **Header:** Camera count & Point count.<br>â€¢ **Cameras:** Focal length ($f$), Distortion ($k_1, k_2$), Rotation ($R$), Translation ($t$).<br>â€¢ **Structure:** 3D landmarks $(X, Y, Z)$, colors, and the list of 2D feature observations per point. | **Core Input:** Parsed to create the initial `gtsam.Values` (Pose3, Point3) and the `gtsam.NonlinearFactorGraph`. |\n",
    "| **`colmap_bundle.out.list.txt`** | **The Association Index**<br>â€¢ A plain text list mapping the internal Bundle index (e.g., Camera 0) to the original image filename (e.g., `image_0005.jpg`). | **Metadata:** Used to verify the ordering of cameras and associate optimized poses back to the source images for visualization. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab774b9b",
   "metadata": {},
   "source": [
    "## GTSAM test bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c919e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Load the Full Data\n",
    "bundle_file = \"colmap_bundle.out.bundle.out\"\n",
    "list_file = \"colmap.bundle.out.list.txt\"\n",
    "\n",
    "print(\"Parsing Bundler file...\")\n",
    "cameras, points, observations = helper.read_bundler_file(bundle_file, list_file)\n",
    "print(f\"Total Cameras: {len(cameras)}\")\n",
    "print(f\"Total Points: {len(points)}\")\n",
    "\n",
    "# Build the full GTSAM objects first (efficient in memory vs re-parsing)\n",
    "full_graph, full_estimate = helper.build_bundler_gtsam_graph(cameras, points, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Scaling Steps\n",
    "# Define the number of images (cameras) for each test run\n",
    "# Adjust these numbers based on your total cameras. \n",
    "# Example: [10, 50, 100, 200, 500]\n",
    "step_sizes = np.linspace(10, len(cameras), num=5, dtype=int) \n",
    "results = []\n",
    "\n",
    "print(f\"Running scaling tests on steps: {step_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run Incremental Tests\n",
    "for n_cams in step_sizes:\n",
    "    print(f\"\\n--- Testing with {n_cams} cameras ---\")\n",
    "    \n",
    "    # 1. Create Subgraph\n",
    "    sub_graph, sub_estimate = helper.create_subgraph(full_graph, full_estimate, n_cams)\n",
    "    num_params = sub_estimate.size()\n",
    "    num_factors = sub_graph.size()\n",
    "    \n",
    "    # 2. Get Sparsity Metric\n",
    "    # We use the number of entries in the linearized Hessian as a proxy for Schur sparsity complexity\n",
    "    try:\n",
    "        linear_graph = sub_graph.linearize(sub_estimate)\n",
    "        hessian = linear_graph.augmentedHessian() # Dense matrix output - CAREFUL on large sizes\n",
    "        # For large scaling, simply counting factor connections is safer than building dense Hessian\n",
    "        # Using Graph size as proxy for 'sparsity' related load\n",
    "        sparsity_metric = linear_graph.keys().size() # simple node count\n",
    "    except Exception as e:\n",
    "        sparsity_metric = 0\n",
    "        print(\"Skipping Hessian formation (too large)\")\n",
    "\n",
    "    # 3. Optimize with Resource Monitoring\n",
    "    params = gtsam.LevenbergMarquardtParams()\n",
    "    params.setVerbosityLM(\"SUMMARY\")\n",
    "    optimizer = gtsam.LevenbergMarquardtOptimizer(sub_graph, sub_estimate, params)\n",
    "\n",
    "    try:\n",
    "        with helper.ResourceMonitor() as monitor:\n",
    "            result = optimizer.optimize()\n",
    "        \n",
    "        stats = monitor.get_stats()\n",
    "        \n",
    "        results.append({\n",
    "            \"num_images\": n_cams,\n",
    "            \"num_points\": len([k for k in range(sub_graph.size())]), # approx\n",
    "            \"ram_max_mb\": stats['max_ram_mb'],\n",
    "            \"cpu_avg\": stats['avg_cpu_percent'],\n",
    "            \"time_sec\": stats['time_sec'],\n",
    "            \"sparsity_proxy\": sparsity_metric\n",
    "        })\n",
    "        print(f\"Completed in {stats['time_sec']:.2f}s, Max RAM: {stats['max_ram_mb']:.2f}MB\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Optimization FAILED/OOM for {n_cams} cameras: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize Results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Images')\n",
    "ax1.set_ylabel('Time (s)', color=color)\n",
    "ax1.plot(df['num_images'], df['time_sec'], color=color, marker='o', label='Time')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Max RAM (MB)', color=color)  \n",
    "ax2.plot(df['num_images'], df['ram_max_mb'], color=color, marker='x', linestyle='--', label='RAM')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Scaling Limits: 1DSfM Dataset\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sparsity Plot\n",
    "plt.figure()\n",
    "plt.plot(df['num_images'], df['sparsity_proxy'], marker='o')\n",
    "plt.title(\"Sparsity (Graph Nodes) vs Images\")\n",
    "plt.xlabel(\"Number of Images\")\n",
    "plt.ylabel(\"System Size (Variables)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c917e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
